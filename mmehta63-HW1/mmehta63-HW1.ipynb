{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 7641 - Supervised Learning\n",
    "## Manish Mehta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file provides the code base for each of the 5 classification algorithms for each of the two datasets. All the analysis in the report are based on this (Running this Notebook generates the same outputs as referred to in the report)\n",
    "\n",
    "Datasets: Phishing Websites, Bank Marketing.\n",
    "\n",
    "Classification Algorithms: Decision Tree, Neural Network, Boosting, Support Vector Machines, k-Nearest Neighbors.\n",
    "\n",
    "The learning curve is plotted, along with the plot for model complexity, after hyperparameter tuning has been performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please save the datasets to your local machine and change the current directory to a file where you have the data stored (for the purpose of this assignment, we can assume that the location of the notebook is same as the location of the 2 CSV files containing the Bank Marketing and Phishing Websites data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Marketing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Bank Marketing Data\n",
    "df_bank_orig = pd.read_csv('BankMarketingData.csv')\n",
    "print(\"Data has\",len(df_bank_orig),\"rows and\", len(list(df_bank_orig)),\"columns.\")\n",
    "if df_bank_orig.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy the original dataset for further processing\n",
    "df_bank = df_bank_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset has columns with various non-numeric levels (eg. 'job' has levels like 'student', 'admin', 'technician', etc.), we will have convert all those categorical columns using one-hot encoding. Standardization of all of the numeric features is also required. We will rewrite the target variables from {no,yes} to {0,1}.\n",
    "\n",
    "Also, the feature 'pdays' is numeric but contains values that are '999' representing if the customer was not called before. For the sake of better interpretability, we can create a new feature that defines whether or not a customer had been called before denoted by {0,1} respectively. Otherwise '999' may be outlier during standardization of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Data\n",
    "col_1hot = df_bank.drop(df_bank.describe().columns.tolist(), axis=1).columns.tolist()[:-1]\n",
    "df_1hot = df_bank[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "df_others = df_bank.drop(col_1hot,axis=1)\n",
    "df_bank = pd.concat([df_others,df_1hot],axis=1)\n",
    "column_order = list(df_bank)\n",
    "column_order = column_order[:-1] + column_order[-1:]\n",
    "\n",
    "df_bank = df_bank.loc[:, column_order]\n",
    "df_bank['y'].replace(\"no\",0,inplace=True)\n",
    "df_bank['y'].replace(\"yes\",1,inplace=True)\n",
    "df_bank['y'] = df_bank['y'].astype('category')\n",
    "\n",
    "numericcols = df_bank.describe().columns.tolist()\n",
    "df_num = df_bank[numericcols]\n",
    "df_stand =(df_num-df_num.min())/(df_num.max()-df_num.min())\n",
    "df_bank_categorical = df_bank.drop(numericcols,axis=1)\n",
    "df_bank = pd.concat([df_bank_categorical,df_stand],axis=1)\n",
    "df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garbaage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phishing Website Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Phishing Websites data\n",
    "\n",
    "df_phish_orig = pd.read_csv('PhishingWebsitesData.csv').astype('category')\n",
    "print(\"Data has\",len(df_phish_orig),\"rows and\", len(list(df_phish_orig)),\"columns.\")\n",
    "if df_phish_orig.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original data into another dataframe for further use\n",
    "df_phish = df_phish_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to do some preprocessing. A lot of columns are categorical with the levels {-1,0,1} and the rest are all binary with levels {-1,1}. We can rewrite the {-1,1} columns to {0,1} and for the 3-level columns, we will use one-hot encoding to create additional features which have level {0,1}. This leads to an increase in the number of features, and all are binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the list of columns which have three categories so that they can be converted to multiple columns with one-hot encoding\n",
    "temp1 = pd.DataFrame(df_phish.describe())\n",
    "idx_uniq = list(temp1.index).index('unique')\n",
    "col_1hot = temp1.columns[temp1.iloc[idx_uniq] == 3].tolist()\n",
    "df_1hot = df_phish[col_1hot]\n",
    "\n",
    "# Convert categorical to dummy variables, same as one-hot encoding\n",
    "df_1hot = pd.get_dummies(df_1hot)\n",
    "# Non-categorical variables/columns separated\n",
    "df_others = df_phish.drop(col_1hot,axis=1)\n",
    "\n",
    "\n",
    "df_phish = pd.concat([df_1hot,df_others],axis=1)\n",
    "df_phish = df_phish.replace(-1,0).astype('category')\n",
    "column_order = df_phish.columns.tolist()\n",
    "# Getting response variable to be the first in the dataframe\n",
    "column_order = column_order[-1:] + column_order[:-1]  \n",
    "\n",
    "df_phish = df_phish.loc[:, column_order]\n",
    "df_phish.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a file with no missing data in the format [y, X] where all features are binary {0,1}. The phishing data is ready to go! Now we move on to loading the Bank Marketing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Useful Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def import_data():\n",
    "\n",
    "    X1 = np.array(df_phish.values[:,1:],dtype='int64')\n",
    "    Y1 = np.array(df_phish.values[:,0],dtype='int64')\n",
    "    X2 = np.array(df_bank.values[:,1:],dtype='int64')\n",
    "    Y2 = np.array(df_bank.values[:,0],dtype='int64')\n",
    "\n",
    "    return X1, Y1, X2, Y2\n",
    "\n",
    "# Plot the learning curve for different training sizes\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    \n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = [] #model performance score (f1)\n",
    "    cv_mean = []; cv_std = [] #model performance score (f1)\n",
    "    fit_mean = []; fit_std = [] #model fit/training time\n",
    "    pred_mean = []; pred_std = [] #model test/prediction times\n",
    "    train_sizes=(np.linspace(.05, 1.0, 20)*n).astype('int')  \n",
    "    \n",
    "    for i in train_sizes:\n",
    "        idx = np.random.randint(X.shape[0], size=i)\n",
    "        X_subset = X[idx,:]\n",
    "        y_subset = y[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        # Find the mean (of all metrics) for all cv splits, for each size of subset\n",
    "        train_mean.append(np.mean(scores['train_score'])); train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score'])); cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time'])); fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time'])); pred_std.append(np.std(scores['score_time']))\n",
    "    \n",
    "    # Calculate the mean value for all the metrics across all subset sizes\n",
    "    train_mean = np.array(train_mean); train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean); cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean); fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean); pred_std = np.array(pred_std)\n",
    "    \n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "    \n",
    "    return train_sizes, train_mean, cv_mean, fit_mean, pred_mean\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Train Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "def NNGridSearchCV(X_train, y_train, h_units, learning_rates):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
    "\n",
    "    nnet = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
    "                       param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    nnet.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(nnet.best_params_)\n",
    "    return nnet.best_params_['hidden_layer_sizes'], nnet.best_params_['learning_rate_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "phishX,phishY, bankX, bankY = import_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for just number of neurons in 1 layer (with only 1 layer possible)\n",
    "param_range = np.linspace(1, 250, 25).astype('int')\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='adam', activation='logistic', learning_rate_init=0.05, random_state=100), \n",
    "    X_train, y_train, param_name='hidden_layer_sizes', param_range=param_range, \n",
    "    cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(param_range, train_scores_mean, 'o-', color='b', label='Train F1 Score')\n",
    "plt.plot(param_range, test_scores_mean, 'o-', color = 'r', label='Test F1 Score')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.ylabel('Model F1 Score')\n",
    "plt.xlabel('No. of neurons in 1 hidden layer structure')\n",
    "\n",
    "plt.title(\"Model Complexity Curve for NN (Phishing Data)\\nHyperparameter : No. of neurons in 1 hidden layer structure\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max validation accuracy obtained at\n",
    "param_range[np.argmax(test_scores_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing as above but using actual test set, not a good way - NOT SURE TO INCLUDE OR NOT?\n",
    "# hyperNN(X_train, y_train, X_test, y_test, \"Model Complexity Curve for NN (Phishing Data)\\nHyperparameter : No. of neurons in 1 hidden layer structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for just number of neurons in multiple layers and neurons\n",
    "param_range = [(100,100,100), (100,50,25), (100,50), (50,100), (50,50), (100), (50), (25)]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='adam', activation='logistic', learning_rate_init=0.05, random_state=100), \n",
    "    X_train, y_train, param_name='hidden_layer_sizes', param_range=param_range, \n",
    "    cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(param_range) + 1), train_scores_mean, 'o-', color='b', label='Train F1 Score')\n",
    "plt.plot(range(1, len(param_range) + 1), test_scores_mean, 'o-', color = 'r', label='Test F1 Score')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.ylabel('Model F1 Score')\n",
    "plt.xlabel('Model Complexity (Decreasing order)')\n",
    "\n",
    "plt.title(\"Model Complexity Curve for NN (Phishing Data)\\nHyperparameter : Different No. of Layer and Neuron Combinations\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max validation accuracy obtained at\n",
    "param_range[np.argmax(test_scores_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for learning rate\n",
    "param_range = np.logspace(-3, -1, 21)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='adam', activation='logistic', hidden_layer_sizes = (100), random_state=100), \n",
    "    X_train, y_train, param_name='learning_rate_init', param_range=param_range, \n",
    "    cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(param_range, train_scores_mean, 'o-', color='b', label='Train F1 Score')\n",
    "plt.semilogx(param_range, test_scores_mean, 'o-', color = 'r', label='Test F1 Score')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.ylabel('Model F1 Score')\n",
    "plt.xlabel('Learning Rate')\n",
    "\n",
    "plt.title(\"Model Complexity Curve for NN (Phishing Data)\\nHyperparameter : Learning Rate\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max obtained at\n",
    "param_range[np.argmax(test_scores_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the final grid search\n",
    "h_units = [(50,50), (100), (50), (25)]\n",
    "learn_rate = [0.01, 0.015, 0.02, 0.025]\n",
    "\n",
    "h_units, learn_rate = NNGridSearchCV(X_train, y_train, h_units, learn_rate)\n",
    "estimator_phish = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=learn_rate, random_state=100)\n",
    "train_samp_phish, NN_train_score_phish, NN_cv_score_phish, NN_fit_time_phish, NN_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"Neural Net Phishing Data\")\n",
    "print(\"Training Score: \", NN_train_score_phish[1])\n",
    "print(\"CV Score: \", NN_cv_score_phish[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n",
    "estimator_phish.fit(X_train, y_train)\n",
    "loss_phish = estimator_phish.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banking Data Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for just number of neurons in 1 layer (with only 1 layer possible)\n",
    "param_range = np.linspace(1, 300, 15).astype('int')\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='adam', activation='logistic', learning_rate_init=0.05, random_state=100), \n",
    "    X_train, y_train, param_name='hidden_layer_sizes', param_range=param_range, \n",
    "    cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(param_range, train_scores_mean, 'o-', color='b', label='Train F1 Score')\n",
    "plt.plot(param_range, test_scores_mean, 'o-', color = 'r', label='Test F1 Score')\n",
    "plt.ylim(0, 0.6)\n",
    "plt.ylabel('Model F1 Score')\n",
    "plt.xlabel('No. of neurons in 1 hidden layer structure')\n",
    "\n",
    "plt.title(\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : No. of neurons in 1 hidden layer structure\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max validation accuracy obtained at\n",
    "param_range[np.argmax(test_scores_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for just number of neurons in multiple layers and neurons\n",
    "param_range = [(150,150), (170,150), (50,50), (50,25), (150), (100), (50), (25)]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='adam', activation='logistic', learning_rate_init=0.05, random_state=100), \n",
    "    X_train, y_train, param_name='hidden_layer_sizes', param_range=param_range, \n",
    "    cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(param_range) + 1), train_scores_mean, 'o-', color='b', label='Train F1 Score')\n",
    "plt.plot(range(1, len(param_range) + 1), test_scores_mean, 'o-', color = 'r', label='Test F1 Score')\n",
    "plt.ylim(0, 0.8)\n",
    "plt.ylabel('Model F1 Score')\n",
    "plt.xlabel('Model Complexity (Decreasing order)')\n",
    "\n",
    "plt.title(\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : Different No. of Layer and Neuron Combinations\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max validation accuracy obtained at\n",
    "param_range[np.argmax(test_scores_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning for learning rate\n",
    "param_range = np.logspace(-3, -1, 21)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='adam', activation='logistic', hidden_layer_sizes = (50,25), random_state=100), \n",
    "    X_train, y_train, param_name='learning_rate_init', param_range=param_range, \n",
    "    cv=10, scoring='f1', n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(param_range, train_scores_mean, 'o-', color='b', label='Train F1 Score')\n",
    "plt.semilogx(param_range, test_scores_mean, 'o-', color = 'r', label='Test F1 Score')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Model F1 Score')\n",
    "plt.xlabel('Learning Rate')\n",
    "\n",
    "plt.title(\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : Learning Rate\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max obtained at\n",
    "param_range[np.argmax(test_scores_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the final grid search\n",
    "h_units = [(50,25), (50), (10)]\n",
    "learn_rate = [0.0158, 0.02, 0.025, 0.05]\n",
    "\n",
    "h_units, learn_rate = NNGridSearchCV(X_train, y_train, h_units, learn_rate)\n",
    "estimator_bank = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=learn_rate, random_state=100)\n",
    "train_samp_bank, NN_train_score_bank, NN_cv_score_bank, NN_fit_time_bank, NN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Neural Net Banking Data\")\n",
    "print(\"Training Score: \", NN_train_score_bank[-1])\n",
    "print(\"CV Score: \", NN_cv_score_bank[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)\n",
    "estimator_bank.fit(X_train, y_train)\n",
    "loss_bank = estimator_bank.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section for neural network will plot the loss curve for each dataset over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curve\n",
    "plt.figure()\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_phish, 'o-', color=\"b\", label=\"Phishing Data\")\n",
    "plt.plot(loss_bank, 'o-', color=\"r\", label=\"Banking Data\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "def hyperSVM(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    kernel_func = ['linear','poly','sigmoid', 'rbf']\n",
    "    for i in kernel_func:         \n",
    "            if i == 'poly':\n",
    "                for j in [2,3,4,5,6,7,8]:\n",
    "                    clf = SVC(kernel=i, degree=j,random_state=100, gamma='auto')\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred_test = clf.predict(X_test)\n",
    "                    y_pred_train = clf.predict(X_train)\n",
    "                    f1_test.append(f1_score(y_test, y_pred_test))\n",
    "                    f1_train.append(f1_score(y_train, y_pred_train))\n",
    "            else:    \n",
    "                clf = SVC(kernel=i, random_state=100, gamma='auto')\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred_test = clf.predict(X_test)\n",
    "                y_pred_train = clf.predict(X_train)\n",
    "                f1_test.append(f1_score(y_test, y_pred_test))\n",
    "                f1_train.append(f1_score(y_train, y_pred_train))\n",
    "                \n",
    "    xvals = ['linear','poly2','poly3','poly4','poly5','poly6','poly7','poly8','sigmoid','rbf']\n",
    "    plt.plot(xvals, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(xvals, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('Kernel Function')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def SVMGridSearchCV(X_train, y_train, kernels='rbf'):\n",
    "    #parameters to search:\n",
    "    #penalty parameter, C\n",
    "    #\n",
    "    Cs = [0.01, 0.1, 1, 10]\n",
    "    gammas = [0.1, 1, 10]\n",
    "    if kernels != 'rbf':\n",
    "        gammas = [1]\n",
    "        \n",
    "    param_grid = {'C': Cs, 'gamma': gammas, 'kernel': kernels}\n",
    "\n",
    "    clf = GridSearchCV(estimator = SVC(random_state=100),\n",
    "                       param_grid=param_grid, cv=10, n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(clf.best_params_)\n",
    "    return clf.best_params_['C'], clf.best_params_['gamma'], clf.best_params_['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY, bankX, bankY = import_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperSVM(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for SVM (Phishing Data)\\nHyperparameter : Kernel Function\")\n",
    "C_val, gamma_val, kernel_val = SVMGridSearchCV(X_train, y_train)\n",
    "estimator_phish = SVC(C=C_val, gamma=gamma_val, kernel=kernel_val, random_state=100)\n",
    "train_samp_phish, SVM_train_score_phish, SVM_cv_score_phish, SVM_fit_time_phish, SVM_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"SVM Phishing Data\")\n",
    "print(\"Training Score: \", SVM_train_score_phish[-1])\n",
    "print(\"CV Score: \", SVM_cv_score_phish[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperSVM(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for SVM (Banking Data)\\nHyperparameter : Kernel Function\")\n",
    "C_val, gamma_val, kernel_val = SVMGridSearchCV(X_train, y_train, ['rbf','linear'])\n",
    "\n",
    "# estimator_bank = SVC(C=C_val, gamma=gamma_val, kernel=kernel_val, random_state=100)\n",
    "estimator_bank = SVC(C=1, kernel='rbf', random_state=100)\n",
    "train_samp_bank, SVM_train_score_bank, SVM_cv_score_bank, SVM_fit_time_bank, SVM_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"SVM Banking Data\")\n",
    "print(\"Training Score: \", SVM_train_score_bank[-1])\n",
    "print(\"CV Score: \", SVM_cv_score_bank[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "\n",
    "def hyperKNN(X_train, y_train, X_test, y_test, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    klist = np.linspace(1,250,25).astype('int')\n",
    "    for i in klist:\n",
    "        clf = kNN(n_neighbors=i,n_jobs=-1)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        f1_test.append(f1_score(y_test, y_pred_test))\n",
    "        f1_train.append(f1_score(y_train, y_pred_train))\n",
    "        \n",
    "    plt.plot(klist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(klist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Neighbors')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20, random_state=100)\n",
    "hyperKNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for kNN (Phishing Data)\\nHyperparameter : No. Neighbors\")\n",
    "estimator_phish = kNN(n_neighbors=40, n_jobs=-1)\n",
    "train_samp_phish, kNN_train_score_phish, kNN_cv_score_phish, kNN_fit_time_phish, kNN_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"kNN Phishing Data\")\n",
    "print(\"Training Score: \", kNN_train_score_phish[-1])\n",
    "print(\"CV Score: \", kNN_cv_score_phish[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperKNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for kNN (Banking Data)\\nHyperparameter : No. Neighbors\")\n",
    "estimator_bank = kNN(n_neighbors=10, n_jobs=-1)\n",
    "train_samp_bank, kNN_train_score_bank, kNN_cv_score_bank, kNN_fit_time_bank, kNN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"kNN Banking Data\")\n",
    "print(\"Training Score: \", kNN_train_score_bank[-1])\n",
    "print(\"CV Score: \", kNN_cv_score_bank[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def hyperTree(X_train, y_train, X_test, y_test, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    max_depth = list(range(1,31))\n",
    "    for i in max_depth:         \n",
    "            clf = DecisionTreeClassifier(max_depth=i, random_state=100, min_samples_leaf=1, criterion='entropy')\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(max_depth, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(max_depth, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('Max Tree Depth')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "     \n",
    "    \n",
    "def TreeGridSearchCV(start_leaf_n, end_leaf_n, X_train, y_train):\n",
    "    param_grid = {'min_samples_leaf':np.linspace(start_leaf_n,end_leaf_n,20).round().astype('int'), 'max_depth':np.arange(1,20)}\n",
    "\n",
    "    tree = GridSearchCV(DecisionTreeClassifier(random_state=100), param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(tree.best_params_)\n",
    "    return tree.best_params_['max_depth'], tree.best_params_['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperTree(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for Decision Tree (Phishing Data)\\nHyperparameter : Tree Max Depth\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "# Leaf nodes of size [0.5%, 5% will be tested]\n",
    "end_leaf_n = round(0.05*len(X_train)) \n",
    "max_depth, min_samples_leaf = TreeGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_phish = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=100, criterion='entropy')\n",
    "train_samp_phish, DT_train_score_phish, DT_cv_score_phish, DT_fit_time_phish, DT_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"Decision Tree Phishing Data\")\n",
    "print(\"Training Score: \", DT_train_score_phish[-1])\n",
    "print(\"CV Score: \", DT_cv_score_phish[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperTree(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for Decision Tree (Banking Data)\\nHyperparameter : Tree Max Depth\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train)) #leaf nodes of size [0.5%, 5% will be tested]\n",
    "max_depth, min_samples_leaf = TreeGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_bank = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=100, criterion='entropy')\n",
    "train_samp_bank, DT_train_score_bank, DT_cv_score_bank, DT_fit_time_bank, DT_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Decision Tree Banking Data\")\n",
    "print(\"Training Score: \", DT_train_score_bank[-1])\n",
    "print(\"CV Score: \", DT_cv_score_bank[-1])\n",
    "\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def hyperBoost(X_train, y_train, X_test, y_test, max_depth, min_samples_leaf, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    n_estimators = np.linspace(1,250,40).astype('int')\n",
    "    for i in n_estimators:         \n",
    "            clf = GradientBoostingClassifier(n_estimators=i, max_depth=int(max_depth/2), \n",
    "                                             min_samples_leaf=int(min_samples_leaf/2), random_state=100,)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(n_estimators, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(n_estimators, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Estimators')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def BoostedGridSearchCV(start_leaf_n, end_leaf_n, X_train, y_train):\n",
    "    param_grid = {'min_samples_leaf': np.linspace(start_leaf_n,end_leaf_n,3).round().astype('int'),\n",
    "                  'max_depth': np.arange(1,4),\n",
    "                  'n_estimators': np.linspace(10,100,3).round().astype('int'),\n",
    "                  'learning_rate': np.linspace(.001,.1,3)}\n",
    "\n",
    "    boost = GridSearchCV(GradientBoostingClassifier(), param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    boost.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(boost.best_params_)\n",
    "    return boost.best_params_['max_depth'], boost.best_params_['min_samples_leaf'], boost.best_params_['n_estimators'], boost.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phishX,phishY,bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(phishX),np.array(phishY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperBoost(X_train, y_train, X_test, y_test, 3, 50, title=\"Model Complexity Curve for Boosted Tree (Phishing Data)\\nHyperparameter : No. Estimators\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train))\n",
    "max_depth, min_samples_leaf, n_est, learn_rate = BoostedGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_phish = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, \n",
    "                                              n_estimators=n_est, learning_rate=learn_rate, random_state=100)\n",
    "train_samp_phish, BT_train_score_phish, DT_cv_score_phish, BT_fit_time_phish, BT_pred_time_phish = plot_learning_curve(estimator_phish, X_train, y_train,title=\"Boosted Tree Phishing Data\")\n",
    "print(\"Training Score: \", BT_train_score_phish)\n",
    "print(\"CV Score: \", BT_cv_score_phish)\n",
    "\n",
    "final_classifier_evaluation(estimator_phish, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20, random_state=100)\n",
    "\n",
    "hyperBoost(X_train, y_train, X_test, y_test, 3, 50, title=\"Model Complexity Curve for Boosted Tree (Banking Data)\\nHyperparameter : No. Estimators\")\n",
    "start_leaf_n = round(0.005*len(X_train))\n",
    "end_leaf_n = round(0.05*len(X_train))\n",
    "max_depth, min_samples_leaf, n_est, learn_rate = BoostedGridSearchCV(start_leaf_n,end_leaf_n,X_train,y_train)\n",
    "estimator_bank = GradientBoostingClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, \n",
    "                                              n_estimators=n_est, learning_rate=learn_rate, random_state=100)\n",
    "train_samp_bank, BT_train_score_bank, BT_cv_score_bank, BT_fit_time_bank, BT_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Boosted Tree Banking Data\")\n",
    "print(\"Training Score: \", BT_train_score_bank)\n",
    "print(\"CV Score: \", BT_cv_score_bank)\n",
    "\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fit_time(n,NNtime, SMVtime, kNNtime, DTtime, BTtime, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Training Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Training Time (s)\")\n",
    "    plt.plot(n, NNtime, '-', color=\"b\", label=\"Neural Network\")\n",
    "    plt.plot(n, SMVtime, '-', color=\"r\", label=\"SVM\")\n",
    "    plt.plot(n, kNNtime, '-', color=\"g\", label=\"kNN\")\n",
    "    plt.plot(n, DTtime, '-', color=\"m\", label=\"Decision Tree\")\n",
    "    plt.plot(n, BTtime, '-', color=\"k\", label=\"Boosted Tree\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def compare_pred_time(n,NNpred, SMVpred, kNNpred, DTpred, BTpred, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Prediction Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Prediction Time (s)\")\n",
    "    plt.plot(n, NNpred, '-', color=\"b\", label=\"Neural Network\")\n",
    "    plt.plot(n, SMVpred, '-', color=\"r\", label=\"SVM\")\n",
    "    plt.plot(n, kNNpred, '-', color=\"g\", label=\"kNN\")\n",
    "    plt.plot(n, DTpred, '-', color=\"m\", label=\"Decision Tree\")\n",
    "    plt.plot(n, BTpred, '-', color=\"k\", label=\"Boosted Tree\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_learn_time(n,NNlearn, SMVlearn, kNNlearn, DTlearn, BTlearn, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Learning Rates: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.plot(n, NNlearn, '-', color=\"b\", label=\"Neural Network\")\n",
    "    plt.plot(n, SMVlearn, '-', color=\"r\", label=\"SVM\")\n",
    "    plt.plot(n, kNNlearn, '-', color=\"g\", label=\"kNN\")\n",
    "    plt.plot(n, DTlearn, '-', color=\"m\", label=\"Decision Tree\")\n",
    "    plt.plot(n, BTlearn, '-', color=\"k\", label=\"Boosted Tree\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_fit_time(train_samp_phish, NN_fit_time_phish, SVM_fit_time_phish, kNN_fit_time_phish, \n",
    "                 DT_fit_time_phish, BT_fit_time_phish, 'Phishing Dataset')              \n",
    "compare_pred_time(train_samp_phish, NN_pred_time_phish, SVM_pred_time_phish, kNN_pred_time_phish, \n",
    "                 DT_pred_time_phish, BT_pred_time_phish, 'Phishing Dataset')   \n",
    "compare_learn_time(train_samp_phish, NN_train_score_phish, SVM_train_score_phish, kNN_train_score_phish, \n",
    "                 DT_train_score_phish, BT_train_score_phish, 'Phishing Dataset')  \n",
    "\n",
    "\n",
    "\n",
    "compare_fit_time(train_samp_bank, NN_fit_time_bank, SVM_fit_time_bank, kNN_fit_time_bank, \n",
    "                 DT_fit_time_bank, BT_fit_time_bank, 'Banking Dataset')       \n",
    "compare_pred_time(train_samp_bank, NN_pred_time_bank, SVM_pred_time_bank, kNN_pred_time_bank, \n",
    "                 DT_pred_time_bank, BT_pred_time_bank, 'Banking Dataset')           \n",
    "compare_learn_time(train_samp_bank, NN_train_score_bank, SVM_train_score_bank, kNN_train_score_bank, \n",
    "                 DT_train_score_bank, BT_train_score_bank, 'Banking Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
